\chapter{Canonical Forms}

Our goal in this chapter is to break a vector space into `important' subspaces with respect to a polynomial associated with a given linear operator.

Here we'll use $V$ to denote a vector space over $\mathbb{F}$, $T \in \text{End}(V)$, and $V_p := \{ v \in V : p(T)(v) = 0 \}$, where $p \in \mathbb{F}[x]$.

\section{Annihilating Polynomials}

\begin{definition}[Annihilator Set]
	We define $\mathfrak{A}_T := \{ p \in \mathbb{F}[x] : p(T) = 0 \}$ as the \textbf{annihilator set} of $T$.
\end{definition}

It follows that the collection of polynomials $p$ which annihilate $T$ is an \hyperref[def:ideal]{ideal} in the polynomial algebra $\mathbb{F}[x]$.

We'll state the following theorem and proceed with a discussion before proving each of its parts.

\begin{theorem}[Existence of the Minimal Polynomial]\label{thm:minimal-polynomial} \hfill
	\begin{enumerate}
		\item If $\dim (V)$ is finite, then $\mathfrak{A}_T \neq \{ 0 \}$.
		\item If $\mathfrak{A}_T \neq \{ 0 \}$, then there exists a unique monic polynomial $m_T \in \mathbb{F}[x]$ such that $m_T$ divides every element of $\mathfrak{A}_T$.
	\end{enumerate}
\end{theorem}

\begin{definition}[Minimal Polynomial]
	The polynomial $m_T$ in the previous theorem is called \textbf{minimal polynomial} of $T$.
	
	If $\mathfrak{A}_T = \{ 0 \}$, we define $m_T = 0$.
\end{definition}

Since every polynomial ideal consists of all multiples of some fixed monic polynomial (the generator of the ideal), we may define the minimal polynomial for $T$ as the unique monic generator of the ideal of polynomials over $\mathbb{F}$ which annihilate $T$.

The next example shows that it is possible to have $\mathfrak{A}_T = \{ 0 \}$ when the dimension is infinite.

\begin{example}
	If $V = \mathbb{F}[x]$ and $T$ is the operator $f(t) \mapsto tf(t)$, then $\mathfrak{A}_T = \{ 0 \}$.
\end{example}

Summarizing, the minimal polynomial $p$ for the linear operator $T$ is uniquely determined by these three properties:
\begin{enumerate}
	\item $p$ is a monic polynomial over $\mathbb{F}$;
	\item $p(T) = 0$;
	\item No polynomial over $\mathbb{F}$ which annihilates $T$ has smaller degree than $p$ has.
\end{enumerate}

These definitions can be easily extended to a matrix $A$ instead of an operator $T$. Moreover, it follows from previous remarks that similar matrices have the same minimal polynomial.

\begin{theorem}
	The characteristic and minimal polynomials have the same roots, except for multiplicities.
\end{theorem}

\begin{proof}
	Let $T \in \text{End}(V)$, where $\dim V = n$, $m_T$ the minimal polynomial for $T$ and $\lambda$ a scalar. We'll show that $m_T(\lambda) = 0$ iff. $\lambda$ is a characteristic value of $T$.

	Suppose that $m_T(\lambda) = 0$. Then write $m_T = (x-\lambda)q(x)$. By the minimality of $m_T$, it follows that $q(T) \neq 0$ and, therefore, there exists $u \in V$ such that $q(T)(u) \neq 0$.
	
	If $v := q(T)(u)$,
	\[
		0 = m_T(T)(u) = (T - \lambda I)(q(T)(u)) = (T - \lambda I)(v)
	\]
	and hence $v$ is an eigenvector of $T$ associated with $\lambda$. Thus, $c_T(\lambda) = 0$.

	Suppose that $\lambda$ is an eigenvalue. Then we can write $m_T(T)v = m_T(\lambda)v$, which implies that $m_T(\lambda) = 0$.
\end{proof}

The \hyperref[thm:cayley-hamilton]{Cayley-Hamilton Theorem} will narrow the search for the minimal polynomial of various operators.

\begin{theorem}[Cayley-Hamilton]\label{thm:cayley-hamilton}
	Let $T \in \text{End}(V)$, where $V$ is finite-dimensional. If $c_T$ is a characteristic polynomial for $T$, then
	\[
		c_T(T) = 0
	\]

	Put another way, the minimal polynomial divides the characteristic polynomial for $T$.
\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item Take a basis $\beta$ of $V$ and write $A = [T]_\beta$.
		\item Consider $A' = xI - A$ and notice that $c_T(x) = \det A'$.
		\item Let $B$ be the adjoint matrix of $A'$. Note that its elements are polynomials of $x$ of degree at most $n-1$.
		\item Write $b_{ij} = b_{ij}^{(0)} + b_{ij}^{(1)}x + \cdots + b_{ij}^{(n-1)}x^{n-1}$.
		\item Let $B^{(k)}$ be the matrix whose entries are given by $b_{ij}^{(k)}$.
		\item Write $c_T(x) = a_0 + a_1 x + \cdots + x^n$ and use that \[ B A' = \text{adj}(A') A' = (\det A') I = c_T(x) I \]
		\item It follows that $(B^{(0)} + B^{(1)} x + \cdots + B^{(n-1)} x^{n-1})(xI - A) = (a_0 + a_1 x + \cdots + x^n) I$.
		\item Comparing each coeficient, we have that
		\[
			\begin{cases}
				a_0 I &= -B^{(0)}A \\
				a_1 I &= B^{(0)} - B^{(1)}A \\
				&~\vdots \\
				a_{n-1} I &= B^{(n-2)} - B^{(n-1)}A \\
				I &= B^{(n-1)}
			\end{cases}
		\]
		\item Multiplying these equations by $I, A, A^2, \ldots, A^n$ respectively, and adding all of them, we have \[ c_T(T) = a_0 I + a_1 A + \cdots + A^n = 0 \]
	\end{enumerate}
\end{proof}

\section{Invariant Subspaces}\label{sec:inv_sub}

Suppose $T \in \text{End}(V)$. If we decompose $V$ in direct sums
	\[
		V = U_1 \oplus \cdots \oplus U_m
	\]
where each $U_j$ is a proper subspace of $V$, then to understand the behavior of $T$ it is sufficient to study the behavior of $T$ in each $U_j$. To do that, we consider $T|_{U_j}$, i.e., $T$ restricted to $U_j$.

However, it is not always the case that $T|_{U_j}$ has its image in the same subspace ${U_j}$. That's why we need the following.

\begin{definition}[Invariant Subspace]
	A subspace $U$ of $V$ is said an $\textbf{invariant subspace}$ under $T$ if for each $u \in U$ we have that $T(u) \in U$. I.e., $U$ is invariant under $T$ if $T|_U$ is a linear operator in $U$.
\end{definition}

\begin{example}
	Each of the following subspaces is invariant under $T$:
	\begin{enumerate}
		\item $\{ 0 \}$;
		\item $V$;
		\item $\ker T$;
		\item $\text{range}~T$.
	\end{enumerate}
\end{example}

With the idea of invariant subspaces, the minimal polynomial gives us characterizations of diagonalizable and triangulable operators.

\begin{lemma}
	Let $W$ be an invariant subspace of $T$, $c_T$ denote the characteristic polynomial for $T$ and $m_T$ the minimal polynomial for $T$. Then
	\[
		c_{T|_{W}} | c_T \quad \text{ and } \quad m_{T|_{W}} | m_T
	\]
\end{lemma}


\begin{lemma}
	Let $W$ be an invariant subspace of $T$. Then, $W$ is invariant under every polynomial in $T$. And, for every $v \in V$, $S(v, W)$ is an ideal in $\mathbb{F}[x]$.
\end{lemma}

\begin{definition}[Triangulable]
	A linear operator $T$ is \textbf{triangulable} if there exists a basis of $V$ under which $T$ is represented as a triangular matrix.
\end{definition}

\begin{lemma}
	Let $V$ be finite-dimensional and that the minimal polynomial $m_T$ for $T \in \text{End}(V)$ is a product of linear factors 
	\[
		m_T = (\lambda - \lambda_1)^{r_1} \cdots (\lambda - \lambda_k)^{r_k}
	\]

	And let $W$ be a proper subspace of $V$ invariant under $T$. Then there exists a vector $v \in V$ such that
	\begin{enumerate}
		\item $v \notin W$;
		\item $(T - \lambda I) v$ is in $W$, for some eigenvalue $\lambda$ of $T$.
	\end{enumerate}
\end{lemma}

\begin{theorem}
	Let $V$ be finite-dimensional. Then $T \in \text{End}(V)$ is triangulable iff. the minimal polynomial $m_T$ is a product of linear polynomials.
\end{theorem}

\begin{proof}
	$(\Rightarrow)$ The idea here is to apply the lemma repeatedly.

	For $W_0 = \{ 0 \}$, there exists $v_0 \neq 0$ such that $(T - \lambda_0 I)v_0 \in W_0$.

	For $W_1 = \text{span}\{ v_0 \}$, there exists $v_1 \notin W_1$ such that $(T - \lambda_1 I)v_1 \in W_1$.

	Repeating this process, we obtain that $T(v_i) \in W_i$, for all $i$, and hence we constructed the desired basis. 

	$(\Leftarrow)$ If $T$ is triangulable, then $c_T$ can be written as
	\[
		c_T = (\lambda - \lambda_1)^{d_1} \cdots (\lambda - \lambda_k)^{d_k}
	\]

	Hence, the elements in the diagonal $a_{11}, \ldots, a_{nn}$, are the eigenvalues. Since $m_T | c_T$, the minimal polynomial can be factored by the eigenvalues $\lambda_i$.
\end{proof}

\begin{corollary}
	If $\mathbb{F}$ is algebraically closed, then every square matrix is similar to a triangular matrix.
\end{corollary}

\begin{theorem}
	Let $V$ be finite-dimensional. Then $T$ is diagonalizable iff. the minimal polynomial for $T$ is a product of distinct linear factors.
	\[
		m_T = (\lambda - \lambda_1) \cdots (\lambda - \lambda_k) \quad ~\lambda_i \neq \lambda_j, ~\forall~i \neq j
	\]
\end{theorem}

\section{Simultaneous Triangulation / Diagonalization}

In this section, suppose that $V$ is finite-dimensional and let $\mathfrak{F}$ denote a family of linear operators on $V$. The main question here is how to diagonalize or triangulize every $T \in \mathfrak{F}$ simultaneously?

Notice that for diagonalization, it is sufficient and necessary that $\mathfrak{F}$ is commuting, since diagonal matrices commute.

\begin{lemma}
	Let $\mathfrak{F}$ be a family of commuting and triangulable operators, and $W \subsetneq V$ invariant under $\mathfrak{F}$ (i.e. invariant under each operator in $\mathfrak{F}$). There exists $v \in V$ such that
	\begin{enumerate}
		\item $v \notin W$;
		\item For all $T \in \mathfrak{F}$, $T(v) \in \text{span}\{ v, W \}$.
	\end{enumerate}
\end{lemma}

\begin{theorem}
	Let $\mathfrak{F}$ be a family of commuting and triangulable operators. There exists an ordered basis for $V$ such that every operator in $\mathfrak{F}$ is represented by a triangular matrix in that basis.
\end{theorem}

\begin{corollary}
	Let $\mathfrak{F}$ be a family of commuting and triangulable operators. If $\mathbb{F}$ is algebraically closed, there exists a matrix $P$ non-singular such that $P^{-1} A P$ is upper triangular, for all $A \in \mathfrak{F}$.
\end{corollary}

\begin{theorem}
	Let $\mathfrak{F}$ be a family of commuting and diagonalizable operators. There exists an ordered basis for $V$ such that every operator in $\mathfrak{F}$ is represented in that basis by a diagonal matrix.
\end{theorem}

The proofs in this section are analogous to the corresponding results in the previous one.

\section{Direct-Sum Decompositions}

To continue our studies, we'll think less in terms of matrices and more in terms of subspaces. Our goal is to decompose a vector space into the sum of invariant subspaces where the restriction is simple.

\begin{definition}[Independent Subspaces]
	Let $W_1, \ldots, W_k$ be subspaces of $V$. Then $W_1, \ldots, W_k$ are \textbf{independent} if 
	\[
		v_1 + \cdots + v_k = 0, \quad v_i \in W_i
	\]
	implies that each $v_i = 0$.
\end{definition}

\begin{lemma}
	Let $V$ be finite-dimensional, $W_1, \ldots, W_k$ subspaces of $V$ and $W = W_1 + \cdots + W_k$. The following are equivalent.
	\begin{enumerate}
		\item $W_1, \ldots, W_k$ are independent.
		\item For all $2 \leq j \leq k$, we have \[ W_j \cap (W_1 + \cdots + W_{j-1}) = \{ 0 \} \]
		\item If $\beta_i$ is an ordered basis for $W_i$, $1 \leq i \leq k$, then the sequence $\beta = (\beta_1, \ldots, \beta_k)$ is an ordered basis for $W$.
	\end{enumerate}
\end{lemma}

Remark that if the conditions of the lemma above hold, then $W = W_1 \oplus \cdots \oplus W_k$. For example, if $W_i$ is the eigenspace associated with the eigenvalue $\lambda_i$ of $T$ and $T$ is diagonalizable, then $V$ can be decomposed as $V = W_1 \oplus \cdots \oplus W_k$.

\begin{theorem}\label{thm:202211101445}
	If $V = W_1 \oplus \cdots \oplus W_k$, then there exist $k$ endomorphisms $P_1, \ldots, P_k$ on $V$ such that
	\begin{enumerate}
		\item Each $P_i$ is a projection (i.e. $P_i^2 = P_i$);
		\item If $i \neq j$, then $P_i P_j = 0$;
		\item $I = P_1 + \cdots + P_k$;
		\item The range of $P_i$ is $W_i$.
	\end{enumerate}

	Conversely, if $P_1, \ldots, P_k$ are $k$ endomorphisms on $V$ satisfying conditions $(1)$, $(2)$, and $(3)$, and if we let $W_i$ be the range of $P_i$, then $V = W_1 \oplus \cdots \oplus W_k$.
\end{theorem}

\begin{proof}
	Certainly, $V = W_1 + \cdots + W_k$. 

	By condition $(3)$, $\alpha = P_1 \alpha + \cdots + P_k \alpha$. But since each $P_i \alpha$ is in $W_i$, we have that $\alpha = \alpha_1 + \cdots + \alpha_k$, with each $\alpha_i \in W_i$. 

	Hence, using $(1)$ and $(2)$ we see that $V$ is the direct sum of the $W_i$.
\end{proof}

\section{Invariant Direct Sums}

The most interesting decomposition $V = W_1 \oplus \cdots \oplus W_k$ is when each subspace $W_i$ is invariant under a given endomorphism $T$. In this case, $T$ induces another endomorphism $T_i$ on each $W_i$, which is simply the restriction of $T$ on $W_i$.

If $v \in V$, then
\[
	v = v_1 + \cdots + c_k, \quad v_i \in W_i
\]
and 
\[
	T(v) = T_1(v_1) + \cdots + T_k(v_k)
\]

\begin{theorem}
	Let $T, V, W_1, \ldots, W_k, P_1, \ldots, P_k$ be as in Theorem \ref{thm:202211101445}. Then each subspace $W_i$ is invariant under $T$ iff. $T$ commutes with each of the projection $P_i$, i.e., $T \circ P_i = P_i \circ T$, $i = 1, \ldots, k$.
\end{theorem}

\begin{proof}
	$(\Leftarrow)$ Suppose that $T$ commutes with each $P_i$ and let $w_j \in W_j$. Then
	\[
		T(w_j) = T(P_j(w_j)) = P_j(T(w_j)) \implies T(w_j) \in \text{range}(P_j)
	\]
	i.e., $W_j$ is invariant under $T$.

	$(\Rightarrow)$ Suppose that each $W_i$ is invariant under $T$. Let $v \in V$. Then 
	\[
		v = P_1(v) + \cdots + P_k(v) \implies T(v) = T(P_1(v)) + \cdots + T(P_k(v))
	\]

	Since $P_i(v) \in W_i$, which is invariant under $T$, we have $T(P_i(v)) = P_i(u_i)$ for some $u_i$. Thus
	\[
		P_j(T(v)) = P_j(T(P_1(v))) + \cdots + P_j(T(P_k(v))) = P_j(u_j) = T(P_j(v))
	\]

	Hence, $P_j(T) = T(P_j)$.
\end{proof}

With this result that invariant direct sum decompositions can be described as projections which commute with $T$, we can start studying deeper decomposition theorems.

\begin{theorem}
	Let $T \in \text{End}(V)$, $\dim(V) < \infty$. If $T$ is diagonalizable and $\lambda_1, \ldots, \lambda_k$ are the distinct eigenvalues of $T$, then there exist endomophisms $P_1, \ldots, P_k$ on $V$ such that
	\begin{enumerate}
		\item $T = \lambda_1 P_1 + \cdots + \lambda_k P_k$;
		\item $I = P_1 + \cdots + P_k$;
		\item $P_i P_j = 0$, $i \neq j$;
		\item $P_i^2 = P_i$ ($P_i$ is a projection);
		\item The range of $P_i$ is the eigenspace for $T$ associated with $\lambda_i$.
	\end{enumerate}

	Conversely, if there exist $\lambda_1, \ldots, \lambda_k \in \mathbb{F}$ and non-zero endomorphisms $P_1, \ldots, P_k$ satisfying conditions $(1)$, $(2)$, and $(3)$, then $T$ is diagonalizable, $\lambda_1, \cdots, \lambda_k$ are the distinct eigenvalues of $T$, and conditions $(4)$ and $(5)$ also hold.
\end{theorem}

\begin{proof}
	$(\Rightarrow)$ Let $W_i$ be the space of eigenvectors associated with the eigenvalue $\lambda_i$. Then $V = W_1 \oplus \cdots \oplus W_k$ and let $P_i$ be the projections associated with this decomposition. By Theorem \ref{thm:202211101445} the conditions $(2)$, $(3)$, $(4)$, and $(5)$ are satisfied. To verify $(1)$, notice that for each $v \in V$,
	\[
		T(v) = T(P_1(v)) + \cdots + T(P_k(v)) = \lambda_1 P_1(v) + \cdots + \lambda_k P_k(v)
	\]

	$(\Leftarrow)$ Since $P_i P_j = 0$ for $i \neq j$, multiplying both sides of $I = P_1 + \cdots + P_k$ by $E_i$, we have that $P_i^2 = P_i$. 

	Multiplying $T = \lambda_1 P_1 + \cdots + \lambda_k P_k$ by $E_i$ implies that $T(P_i) = \lambda_i P_i$ and therefore any vector in the range of $P_i$ is in $\ker (T - \lambda_i I)$. Since $P_i \neq 0$, there exists a non-zero vector in $\ker (T - \lambda_i I)$, i.e., $\lambda_i$ is an eigenvalue of $T$.
	
	More than that, $\lambda_i$ are all the eigenvalues of $T$. If $x$ is any scalar then $(T - xI)v = 0$ implies that $(\lambda_i - x)P_i(v) = 0$. If $v \neq 0$, then $P_i(v) \neq 0$ for some $i$. 
	
	Since every non-zero vector is in the range of $P_i$ is an eigenvector of $T$ and $I = P_1 + \cdots + P_k$, we have that these eigenvectors span $V$. 

	Now notice that if $T(v) = \lambda_i v$ then 
	\[
		\sum_{j=1}^k (\lambda_j - \lambda_i) P_j(v) = 0 \iff (\lambda_j - \lambda_i) P_j(v) = 0, \quad \forall~j
	\]
	Thus
	\[
		P_j(v) = 0, \quad j \neq i
	\]

	Since $v = P_1(v) + \cdots + P_k(v)$ and $P_j(v) = 0$ for $j \neq i$, then $v = P_i(v)$, which shows that $v \in \text{range}(P_i)$.
\end{proof}

\section{Primary Decomposition Theorem}\label{sec:pdt}

Suppose that the minimal polynomial for $T$ is of the form
\[
  m_T = (x - \lambda_1)^{r_1} \cdots (x - \lambda_k)^{r_k}
\]
where $\lambda_1, \ldots, \lambda_k$ are distinct scalars. The next theorem shows that the whole space $V$ is the direct sum of the null spaces of $(T - \lambda_i I)^{r_i}$, for $i = 1, \ldots, k$. In fact, the theorem is more general, as we'll see.

\begin{theorem}[Primary Decomposition Theorem (PDT)]\label{thm:pdt}
  Let $T \in \text{End}(V)$, $\dim(V) < \infty$, and $p_1, \ldots, p_m$ be the distinct irreducible factors of $m_T$ in $\mathbb{F}[x]$. If $r_i$ is the multiplicity of $p_i$ in $m_T$, then
  \begin{enumerate}
    \item $V = V_{p_1^{k_1}} \oplus \cdots \oplus V_{p_m^{k_m}}$;
    \item Each $V_{p_i^{k_i}}$ is invariant under $T$;
    \item If $T_i = T|_{W_i}$, i.e., $T_i$ is the operator induced on $W_i$ by $T$, then the minimal polynomial for $T_i$ is $p_i^{r_i}$.
  \end{enumerate}
	The polynomials $p_i^{k_i}$ are called the \textbf{primary factors (pf)} of $T$ and $V_{p_i^{k_i}}$ is said to be a \textbf{$T$-primary subspace} of $V$.
\end{theorem}

In words, if there is non-zero polynomial in the annihilator, then there exists a minimal polynomial. Considering the prime factors of the minimal polynomial and its multiplicities, we can `break' the vector space as direct sum of certain subspaces, where each subspace is related to one of the factors of the minimal polynomial to its multiplicity.

It follows immediately from the PDT that a linear operator is diagonalizable iff. its primary factors have degree one.

\begin{corollary}
  If $P_1, \ldots, P_k$ are projections associated with the primary decomposition of $T$, then each $P_i$ is a polynomial in $T$, and if an endomorphism $U$ commutes with $T$, then $U$ commutes with each $P_i$, i.e., each subspace $W_i$ is invariant under $U$.
\end{corollary}

Recall that

\begin{definition}[Nilpotent Operator]
  Let $N \in \text{End}(V)$. Then $N$ is \textbf{nilpotent} if there exists some positive integer $r$ such that $N^r = 0$.
\end{definition}

Under an algebraically closed field, the PDT allows us to `decompose' an endomophism into a \textbf{diagonalizable part} $D$ and a nilpotent operator $N$, i.e., $T = D + N$. Moreover, $D$ and $N$ commute.

\begin{theorem}
  Let $T \in \text{End}(V)$, $\dim(V) < \infty$. Suppose that the minimal polynomial for $T$ decomposes into a product of linear polynomials. Then there exists a diagonalizable operator $D$ and a nilpotent operator $N$ such that 
  \begin{enumerate}
    \item $T = D + N$;
    \item $DN = ND$.
  \end{enumerate}

  $D$ and $N$ are uniquely determined by $(1)$ and $(2)$ and each of them is a polynomial in $T$.
\end{theorem}

\begin{corollary}
  Let $V$ be a finite-dimensional vector space over an algebraically closed field. Then every endomorphism $T$ on $V$ can be written as the sum of a diagonalizable operator $D$ and a nilpotent operator $N$ which commute. These operators $D$ and $N$ are unique and each is a polynomial in $T$.
\end{corollary}

This fact is important because it reduces our work to the study of nilpotent operators. 

\section{Alternative proofs for minimal polynomials}

Recall that a subspace $W$ of $V$ is said to be $T$-invariant if $T(W) \subseteq W$. In particular, the restriction of $T$ to $W$ induces a linear operator in $W$ given by $w \mapsto T(w)$ for all $w \in W$.

\begin{lemma}
	If $S \in \text{End}(V)$ satisfies $S \circ T = T \circ S$, then the nullspace of $S$, $V_S$, is $T$-invariant. In particular, $V_p$ is $T$-invariant for all $p \in \mathbb{F}[x]$.
\end{lemma}

\begin{proof}
	Let $v \in V_S$ and notice that $S(T(v)) = T(S(v)) = 0$, which shows that $T(v) \in V_S$.
\end{proof}

Now, let us try to describe the following set
\begin{equation}\label{eq:set-goal-describe}
	\{ p \in \mathbb{F}[x] : V_p \neq \{ 0 \} \}
\end{equation}

We already know that the minimal polynomial is inside this set. Besides, for any two polynomials $f, p$, then $V_p \subseteq V_{fp}$. In particular, $V_p^k \subseteq V_p^{k+1}$ for all $k \geq 0$. Therefore, the set
\[
	V_p^\infty := \bigcup_{k > 0} V_p^k
\]
is a $T$-invariant subspace of $V$.

\begin{definition}[Generalized Eigenspace]
	For $p(t) = t - \lambda$, where $\lambda \in \mathbb{F}$, the space $V_p^\infty$ is called the \textbf{generalized eigenspace} associated with $\lambda$.
\end{definition}

\begin{example}
	If $T \in \text{End}(\mathbb{F}^2)$ is given by $T(x,y) = (y,0)$ and $p(t) = t$, then
	\[
		V_p = V_T = [e_1] \quad \text{ and } \quad V_p^2 = \mathbb{F}^2 ~\text{ (since $T^2 = 0$)}
	\]
	
	Is there any other polynomial such that $V_p \neq \{ 0 \}$?
	
	If $f \in \mathbb{F}[x]$ and $p \nmid f$, then $V_f = \{ 0 \}$. In fact, if $f = pq+r$ is the division of $f$ by $p$, then $r \in \mathbb{F} \setminus \{ 0 \}$ (since $p$ has degree one and $p \nmid f$) and
	\[
		f(T)(x,y) = q(T)(T(x,y)) + r(x,y) = q(T)(y,0) + r(x,y) = (q(0)y + rx, ry)
	\]
	
	Therefore, if $r \neq 0$, 
	\[
		F(T)(x,y) = (0, 0) \iff 0 = y = x
	\]
\end{example}

The following proof, for the second item of the Theorem \ref{thm:minimal-polynomial}, shows the existence of minimal polynomial and how to find it.

\begin{theorem}
	If $\mathfrak{A}_T \neq \{ 0 \}$, then there exists a unique monic polynomial $m_T \in \mathbb{F}[x]$ such that $m_T$ divides every element of $\mathfrak{A}_T$.
\end{theorem}

\begin{proof}
	Let $m = \min \{ k : \exists p \in \mathfrak{A}_T \setminus \{ 0 \}, \deg(p) = k \} > 0$, i.e., the smallest degree of a non-zero polynomial in the annihilator.
	
	Fix $f, p \in \mathfrak{A}_T$ with $\deg(p) = m$. By Euclid's Division Algorithm, $f = qp + r$, where $\deg(r) < m$. Notice that $r = f - qp \in \mathfrak{A}_T$. By the minimality of $m$, $r = 0$ and therefore $p | f$.
	
	This shows that any non-constant polynomial with the minimal degree divides every other polynomial in the annihilator. And a polynomial divides another polynomial with the same degree if one is a scalar multiple of the other.
\end{proof} 

To describe the set \eqref{eq:set-goal-describe}, it is sufficient to consider the following lemma and proposition.

\begin{lemma}\label{lemma:gcd-restriction}
	If $\gcd(f,g) = 1$, then the restriction of $f(T)$ to $V_g$ is injective.
\end{lemma}

\begin{proof}
	Let $p, q \in \mathbb{F}[x]$ such that $pf + qg = 1$. Then, for every $v \in V_g$,
	\[
		v = (p(T)f(T) + q(T) g(T))(v) = (p(T)f(T))(v)
	\]
	since $g(T)(v) = 0$. It follows that the restriction of $p(T) \circ f(T)$ to $V_g$ is the identity function. Hence, the lemma follows.
\end{proof}

\begin{proposition}
	If $\mathfrak{A}_T \neq \{ 0 \}$ and $p \in \mathbb{F}[x]$ is irreducible, then $V_p \neq \{ 0 \}$ iff. $p | m_T$.
\end{proposition}

\begin{proof}
	Suppose that $p | m_T$ and $V_p = \{ 0 \}$, i.e., $p(T)(u) \neq 0$ for all $u \in V \setminus \{ 0 \}$. Consider $f = m_T / p$ and notice that
	\[
		0 = m_T(T)(v) = p(T)(f(T)(v)) \quad \forall~v \in V
	\]
	
	If $f(T)(v) \neq 0$, then $p(T)(f(T)(v)) \neq 0$. Thus, $f \in \mathfrak{A}_T$, which is a contradiction since $\deg(f) < \deg(m_T)$.
	
	Reciprocally, if $p \nmid m_T$, then $\gcd(p, m_T) = 1$, since $p$ is irreducible. If follows from the lemma that the restriction of $m_T(T)$ to $V_p$ is injective. Since $m_T(T) = 0$, we can conclude that $V_p = \{ 0 \}$. 
\end{proof}

Put another way, a prime polynomial lives in the set iff. it divides the minimal polynomial.

In a finite dimensional vector space, how can we compute the minimal polynomial? Proving the first item of the Theorem \ref{thm:minimal-polynomial}, we show an algorithm of how to do it.

\begin{theorem}
	If $\dim(V) = n < \infty$, then $\mathfrak{A}_T \neq \{ 0 \}$.
\end{theorem}

\begin{proof}
	If $T = 0$, then $\mathfrak{A}_T = \{ p \in \mathbb{F}[x] : p(0) = 0 \} \neq \{ 0 \}$. In this case, the minimal polynomial is $p(t) = t$.
	
	Suppose that $T \neq 0$ and remember that $\dim(\text{End}(V)) = n^2$. Then we can construct the family $(T^k)_k$, where $k = 0, 1, \ldots, n^2$, which is linearly dependent (since it contains $n^2 + 1$ elements).
	
	Since the subfamily given by $\mathfrak{A}_T = I_V$ is linearly independent, there exists $1 \leq m \leq n^2$ minimal such that the subfamily $(T^k)_{k=0, \ldots, m}$ is linearly dependent.
	
	Let $a_0, \ldots, a_{m-1} \in \mathbb{F}$ such that
	\[
		T^m = a_0 I_V + \cdots + a_{m-1} T^{m-1} \text{ and } f(t) = t^m - \sum_{k=0}^{m-1} a_k t^k
	\]
	
	It follows that $f(T) = 0$ and, therefore, $f \in \mathfrak{A}_T \setminus \{ 0 \}$.
\end{proof}

\textbf{Exercise.} Prove that $f = m_T$, where $f$ is as in the preceeding proof. How can we use this fact and matricial representations of $T$ to compute $m_T$?

\section{Cyclic subspaces}

\begin{definition}[$T$-cycle]
	Given $v \in V$, the sequence of vectors
	\[
		v_0 = v, v_1 = T(v), \ldots, v_k = T^k(v), \ldots
	\]
  is called a \textbf{$T$-cycle} generated by $v$. We denote it by \[ \mathfrak{C}_T^\infty (v) = (v_k)_{k \geq 0} \quad \textbf{ and } \quad \mathfrak{C}_T^m (v) = v_0, v_1, \ldots, v_{m-1} \]

	We define the \textbf{$T$-cyclic subspace generated by $v$} as
	\[
		C_T(v) = [\mathfrak{C}_T^\infty (v)]
	\]
\end{definition}

If $\dim(V)$ is finite, there exists $m \geq 0$ minimal such that $\mathfrak{C}_T^{m+1}(v)$ is linearly dependent. Notice that $m \geq 1$ if $v \neq 0$. In particular, if $\mathfrak{C}_T(v) := \mathfrak{C}_T^{m}(v)$ is a basis for $C_T(v)$ and $v_m$ is a linear combination of the linearly independent vectors $v_0, \ldots, v_{m-1}$. And
\[
	v_m = a_0 v_0 + \cdots + a_{m-1} v_{m-1} = \sum_{k=0}^{m-1} a_k T^k(v)
\]

Defining $m_{T,v}(t) = t^m - a_{m-1}t^{m-1} - \ldots - a_1t - a_0$, called the \textbf{minimal polynomial of $v$ with respect to $T$}, 
\[
	m_{T, v}(T)(v) = v_m - \sum_{k=0}^{m-1} a_k T^k(v) = 0
\]
which proves that $v \in V_{m_{T,v}}$. Since $V_{m_{T,v}}$ is $T$-invariant, it follows that
\[
	C_T(v) \subseteq V_{m_{T,v}}
\]

Cyclic subspaces are useful to find divisors of $m_T$. Since $T$ is fixed, we'll write $m_v = m_{T,v}$. 

The next result shows that it is possible to approximate the minimal polynomial of a linear operator by a minimal polynomial of a vector.

\begin{theorem}
	For all $v \in V$, $m_v | m_T$.
\end{theorem}

\begin{proof}
	Since $V_{m_v}$ is $T$-invariant, consider the induced operator
	\[
		S : V_{m_v} \longrightarrow V_{m_v}, \quad v \mapsto T(v)
	\]
	
	Note that $m_v \in \mathfrak{A}_S$. Thus, $m_S | m_v$. In fact, $m_v = m_S$, since if $\deg(m_S) = m' < m$, then $v_0, \ldots, v^{m'}$ would be linearly dependent, contradicting the minimality of $m$.
\end{proof}

We proceed to show that coprimality implies direct sum.

\begin{theorem}[Coprimality implies direct sum]
	If each pair $p_1, \ldots, p_m \in \mathbb{F}[x]$ is relatively prime, then the sum $V_{p_1}^\infty + \cdots + V_{p_m}^\infty$ is direct.
\end{theorem}

\begin{proof}
	Let $v_j \in V_{p_j}^\infty \setminus \{ 0 \}$, for $1 \leq j \leq m$. Our goal is to show that $v_1, \ldots, v_m$ is linearly independent (which is equivalent to show that the sum in the theorem is direct). We proceed by induction on $m$, which is immediate when $m = 1$. 

	Suppose that $a_1, \ldots, a_m \in \mathbb{F}$ satisfies $a_1 v_1 + \cdots + a_m v_m = 0$. By the induction hypothesis, $v_1, \ldots, v_{m-1}$ is linearly independent. 

	For each $1 \leq j \leq m$, choose $k_j$ such that $p_j^{k_j}(T)(v_j) = 0$ and consider 
	\[
		p = \prod_{j=1}^{m-1} p_j^{k_j}
	\]

	Notice that 
	\[
		0 = p(T)(a_1 v_1 + \cdots + a_m v_m) = a_m p(T)(v_m)
	\]

	By the Lemma \ref{lemma:gcd-restriction}, the restriction $p_j^{k_j}(T)$ to $V_{p_m^{k_m}}$ is injective for $j \neq m$, it follows that $p(T)(v_m) \neq 0$ and, therefore, $a_m = 0$. The induction hypothesis implies that $a_j = 0$ for all $j$.
\end{proof}

Finally, we can decompose $V$ into direct sums.

\begin{theorem}
	Let $f_1, \ldots, f_m \in \mathbb{F}[x]$, where each pair is relatively prime and $f = f_1 \ldots f_m$. Then, $V_f = V_{f_1} \oplus \cdots \oplus V_{f_m}$.
\end{theorem}

\begin{proof}
	We'll prove for $m = 2$. The general case follows from induction on $m$.

	Then $V_{f_1} + V_{f_2} \subseteq V_f$ and we just proved that this sum is direct.

	Let $g_1, g_2 \in \mathbb{F}[x]$ such that $g_1 f_1 + g_2 f_2 = 1$, and define $h_j := g_j f_j$ and $P_j := h_j(S)$, where $S$ is the induced linear operator by $T$ on $V_f$ (i.e. the restriction of $T$ to $V_f$). 

	By construction, $f \in \mathfrak{A}_S$ and $P_1 + P_2 = I_{V_f}$. Notice that
	\[
		P_1 P_2 = g_1(S)f_1(S)g_2(S)f_2(S) = g_1(S)g_2(S)f_1(S)f_2(S) = 0 = P_2 P_1
	\]

	In particular, $P_j^2 = P_j - P_i P_j = P_j$ if $\{ i, j \} = \{ 1, 2 \}$ and $\text{range}~(P_j) \subseteq V_{f_i}$. It follows that $V_f = \text{range}~(P_2) \oplus \text{range}~(P_1) \subseteq V_{f_1} \oplus V_{f_2} \subseteq V_f$. Hence, $V_{f_1} + V_{f_2} = V_f$.
\end{proof}

Taking $f_j = p_j^{k_j}$, we prove the \hyperref[thm:pdt]{Primary Decomposition Theorem}, since $V = V_{m_T}$.

However, the PDT breaks the vector space in direct sums that are still quite complicated. Our goal now is to break the vector space into the sum of cyclic subspaces. 

The next theorem states that we can find a finite collection of vectors such that it is possible to break the space into a direct sum of cyclic subspaces. Therefore, we can find a basis formed by the union of cycles, which are easy to operate.

Our main result here, the Cyclic Decomposition Theorem, is a generalization of this result.

Recall that if $v$ is an eigenvector, then $T(v) = \lambda v$. Hence, a basis formed by eigenvectors is a basis formed by union of cycles, where all cycles have size one. 

\begin{lemma}\label{lemma:202210200909}
	If $p \in \mathbb{F}[x]$ is irreducible and $u, v \in V$ satisfying $m_u = m_v = p$. Then one and only one of the following is true:
	\begin{enumerate}
		\item $C_T(u) = C_T(v)$;
		\item $C_T(u) \cap C_T(v) = \{ 0 \}$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	\textbf{Exercise.}
\end{proof}

\begin{lemma}\label{lemma:202210200910}
	$C_T(v) = \{ p(T)(v) : p \in \mathbb{F}[x] \}$.
\end{lemma}

\begin{proof}
	\textbf{Exercise.}
\end{proof}

A subspace $W$ is said to be \textbf{$T$-cyclic} if $W = C_T(v)$ for some $v \in V$.

\begin{theorem}
	If $p \in \mathbb{F}[x]$ is irreducible and $\dim (V_p)$ is finite, there exist $l \geq 0$ and $v_1, \ldots, v_l \in V_p$ such that $V_p = C_T(v_1) \oplus \cdots \oplus C_T(v_l)$.
\end{theorem}

\begin{proof}
	Suppose that we found vectors $v_1, \ldots, v_k \in V_p$ such that $C_T(v_1) + \cdots + C_T(v_k)$ is a direct sum. We will show that 
	\begin{equation}\label{eq:202210200905}
		C_T(v) \cap (C_T(v_1) + \cdots + C_T(v_k)) = \{ 0 \}
	\end{equation}
	for all $v \in V_P \setminus C_T(v_1) + \cdots + C_T(v_k)$. 

	If this is true, since $\dim (V_p) < \infty$, it is immediate how to choose a sequence of vectors starting with $v_1 \in V_p$ arbitrary. 

	To show \eqref{eq:202210200905}, suppose that $w \in C_T(v) \cap (C_T(v_1) + \cdots + C_T(v_k))$ is non-zero. Therefore, there exists $w_j \in C_T(v_j)$ such that $w = w_1 + \cdots + w_k$.

	By the lemma \ref{lemma:202210200909}, $C_T(w) = C_T(v)$ and, by the lemma \ref{lemma:202210200910}, there exists $f \in \mathbb{F}[x]$ such that $v = f(T)(w)$. Hence,
	\[
		v = f(T)(w_1) + \cdots + f(T)(w_k) \in C_T(v_1) + \cdots + C_T(v_k)
	\]
	contradicting our hypothesis about $v$.
\end{proof}

\begin{theorem}
	Let $p \in \mathbb{F}[x]$ be irreducible and $\dim (V_p^\infty) < \infty$. Then $\deg(p) | \dim (V_p^\infty)$. Moreover,
	\[
		\frac{\dim (V_p^\infty)}{\deg(p)} \geq \min \{ k : V_p^\infty = V_{p^k} \}
	\]
	where the equality holds iff. $V_p^\infty$ is $T$-cyclic.
\end{theorem}

\begin{proof}
	Define $m = \min \{ k : V_p^\infty = V_{p^k} \}$. Suppose that $V_p^\infty$ is $T$-cyclic. Let $S$ be the operator $T$ restricted to $V_p^\infty = V_{p^m}$ and notice that $m_S = p^m$.
	
	Moreover, if $v$ satisfies $V_p^\infty = C_T(v)$, then $m_v = m_S = p^m$. Since $\dim(C_T(v)) = \deg (m_v) = m \cdot \deg(p)$, the result follows.

	We proceed by induction on $n = \dim (V_p^\infty)$. For $n = 1$, it is immediate. Suppose that $n > 1$. Our induction hypothesis is: if $S$ is an endomorphism on $W$ such that $\dim (W_{p(S)}^\infty) < n$, then
	\[
		\deg(p) | \dim (W_{p(S)}^\infty) \quad \text{ and } \quad \frac{\dim (W_{p(S)}^\infty)}{\deg(p)} \geq \min \{ k : W_{p(S)}^\infty = W_{p^k} \}
	\]

	For $m = 1$, the cyclic case with the previous theorem complete the proof.
	
	If $m > 1$, consider $W = V_{p^{m-1}}$, which is a $T$-invariant, proper and non-trivial subspace of $V_p^\infty$. Let $S$ be the induced operator by $T$ on $W$ and notice that $W_{p^k(S)} = V_{p^k(T)}$ if $k < m$. Hence, $W = W_{p(S)}^\infty = V_{p^{m-1}}$. By induction hypothesis,
	\[
		\deg(p) | \dim(W) \quad \text{ and } \quad \frac{\dim(W)}{\deg(p)} \geq m-1
	\]
	
	Consider $R$ the operator on $V_p^\infty$ induced by $p^{m-1}(T) = p(T)^{m-1}$. Notice that $W = \ker(R)$. Hence, by the Rank-Nullity Theorem,
	\[
		\dim(V_p^\infty) = \dim(W) + \dim(\text{range}~(R))
	\]
	
	Thus, what remains to be shown is that $\deg(p) | \dim(\text{range}~(R))$. Note that $\text{range}~(R)$ is $T$-invariant: if $w = R(v)$, where $v \in V_p^\infty$, since $R \circ T = T \circ R$, it follows that 
	\[
		T(w) = T(R(v)) = R(T(v)) \in \text{range}~(R)
	\]
	
	More than that, $\text{range}~(R) \subseteq V_p$: if $v \in V_p^\infty$, $p(T)(R(v)) = p^m(T)(v) = 0$. By the induction hypothesis on $\text{range}~(R)$ instead of $W$, the result follows.
\end{proof}

How is all of this related to the idea of characteristic polynomial?

If $\dim(V) = n < \infty$ and $p_j^{k_j}$, where $1 \leq j \leq m$, are the primary factors of $T$. It follows that.
\[
	\deg (m_T) = \sum_{j=1}^m k_j \deg (p_j) \leq \sum_{j=1}^m \dim(V_{{p_j}^{k_j}}) = n
\]
since 
\[
	k_j = \min \{ k : V_{{p_j}^k} = V_{{p_j}^{k+1}} \}
\]

Define 
\[
	n_j := \frac{\dim (V_{{p_j}^{k_j}})}{\deg(p_j)} \geq k_j
\]
and
\[
	c_T := \prod_{j=1}^m p_j^{n_j}
\]

This polynomial $c_T$ is the \textbf{characteristic polynomial} of $T$. By definition, $\deg(c_T) = n$ and $c_T \in \mathfrak{A}_T$ (\hyperref[thm:cayley-hamilton]{Cayley-Hamilton Theorem}).

If $T_j$ is the restriction of $T$ on $V_{{p_j}^{k_j}}$,
\[
	c_T = \prod_{j=1}^m c_{T_j}
\]

With these tools at hand, we can define determinants and the trace. Let $c_k \in \mathbb{F}$, $1 \leq k \leq n$, such that
\[
	c_T(t) = t^n + \sum_{k=1}^n (-1)^k c_k t^{n-k}
\]
and define $\det(T) = c_n$ and $\text{tr}(T) = c_1$.

We will show that $\det(T) = \det([T]_\alpha^\alpha)$ for every basis $\alpha$ of $V$ and
\[
	c_T(\lambda) = \det([T]_\alpha^\alpha - \lambda I)
\]

This gives us a new method to obtain a the minimal polynomial $m_T$.

\begin{enumerate}
	\item Compute $c_T$ and find its irreducible factors (hint: start by row reducing the matrix), say $p_1, \ldots, p_m$. Let $n_j$ be the multiplicity of $p_j$ in the factorization of $c_T$;
	\item Let $A = [T]_\alpha^\alpha$ and, given $k = (k_1, \ldots, k_m) \in (\mathbb{Z}_{>0})^m$ with $k_j \leq n_j$, let $p_k = \sum_{j=1}^m p_j^{k_j}$. From all polynomials of the form $p_k$ such that $p_k(A) = 0$, the one with the minimal $k_j$ for all $j$ is $m_T$. To find each minimum $k_j$, we can compute the sequence of kernels $V_{p_j} \subseteq \ldots \subseteq V_{p_j^{n_j}} = V_{p_j^{n_j + 1}}$.
\end{enumerate}

\begin{example}\label{ex:canonical_forms_1}
	Suppose that $\text{char}(\mathbb{F})$, $V = \mathbb{F}^4$ and $T \in \text{End}(V)$ given by 
	\[
		T(x_1, x_2, x_3, x_4) = (3x_3 + x_4, 2x_1+2x_2 - 4x_3 + 2 x_4, x_3 + x_4, 3x_4 - x_3)
	\]
	
	If $\alpha$ is the standard basis,
	\[
		[T]_\alpha^\alpha = \begin{bmatrix}
		0 && 0 && 3 && 1 \\
		2 && 2 && -4 && 2 \\
		0 && 0 && 1 && 1 \\
		0 && 0 && -1 && 3
		\end{bmatrix}
		\quad \text{ and } \quad c_T(\lambda) = t(t-2)^3
	\]
	
	Therefore, the options for $m_T$ are $t(t-2)^k$ with $1 \leq k \leq 3$. In fact,
	\[
		k = 4 - \dim(V_{t_2})
	\]
	
	Since
	\[
		[T]_\alpha^\alpha - 2I = \begin{bmatrix}
		-2 && 0 && 3 && 1 \\
		2 && 0 && -4 && 2 \\
		0 && 0 && -1 && 1 \\
		0 && 0 && -1 && 1
		\end{bmatrix}
	\]
	the homogeneous linear system associated with this matrix has solution $\{ (0, x, 0, 0) : x \in \mathbb{F} \}$, it follows that $k = 3$ and 
	\[
		[e_2] = V_{t-2} \not\subseteq V_{(t-2)^2} \not\subseteq V_{(t-2)^3} = V_{t-2}^\infty
	\]
	
	Squaring the matrix,
	\[
		\begin{bmatrix}
		-2 && 0 && 3 && 1 \\
		2 && 0 && -4 && 2 \\
		0 && 0 && -1 && 1 \\
		0 && 0 && -1 && 1
		\end{bmatrix}^2
		= \begin{bmatrix}
		4 && 0 && -10 && 2 \\
		-4 && 0 && 8 && 0 \\
		0 && 0 && 0 && 0 \\
		0 && 0 && 0 && 0
		\end{bmatrix}
	\]
	with solution $V_{(t-2)^2} = \{ (2x, y, x, x) : x, y \in \mathbb{F} \}$.
	
	Taking the cube,
	\[
		\begin{bmatrix}
		-2 && 0 && 3 && 1 \\
		2 && 0 && -4 && 2 \\
		0 && 0 && -1 && 1 \\
		0 && 0 && -1 && 1
		\end{bmatrix}^3
		= \begin{bmatrix}
		-8 && 0 && 20 && -4 \\
		8 && 0 && -20 && 4 \\
		0 && 0 && 0 && 0 \\
		0 && 0 && 0 && 0
		\end{bmatrix}
	\]
	with solution $V_{(t-2)^3} = \{ (z, y, x, 5x-2z) : x, y, z \in \mathbb{F} \}$.
	
	Hence, $m_T = t(t-2)^3$.
\end{example}

\section{Jordan and Frobenius Bases}

In this section, we assume that $V$ is a finite-dimensional space.

Frobenius basis: always exist. Jordan basis: only when the prime factors have degree one.

Suppose that $m_v(t) = (t - \lambda)^k$, for $k \in \mathbb{Z}_{\geq 0}$ and recall that $C_T(v)$ is $T$-invariant and $\dim (C_T(v)) = k$.

Define $w_j = (T - \lambda I)^j (v)$, $\beta = w_0, \ldots, w_{k-1}$, and notice that $w_k = 0 = w_{k+1} = \ldots$. Moreover, $m_{w_j} = (t - \lambda)^{k-j}$ for $j < k$. In particular, $\beta$ is linearly independent and, therefore, a basis of $C_T(v)$.

Notice that $T(w_j) = \lambda w_j + w_{j+1}$, and therefore,
\[
	[S]_\beta^\beta = J_k(\lambda) := \begin{bmatrix}
		\lambda	& 0 		& \cdots 	& \cdots 	& 0 \\
		1		& \lambda 	& 0 		& \cdots 	& 0 \\
		0		& \ddots 	& \ddots 	& \ddots 	& \vdots \\
		\vdots	& \ddots	& \ddots 	& \ddots 	& 0 \\
		0		& \cdots	& 0 		& 1 		& \lambda
	\end{bmatrix}
\]
in which $S \in \text{End}(C_T(v))$ is given by $S(w) = T(w)$ for all $w \in C_T(v)$. The matrix $J_k(\lambda)$ is said to be a \textbf{Jordan block} of size $k$ and eigenvalue $\lambda$.

It is sufficient to reorder the basis $\beta$ to change to an upper triangular matrix.

The sequence $\beta$ is a \textbf{Jordan $T$-cycle} of size $k$ and eigenvalue $\lambda$. A basis formed by the union of Jordan $T$-cycles is a \textbf{Jordan basis} (w.r.t. $T$).

If $\beta$ is a Jordan basis of $V$ with respect to $T$ with $l$ $T$-cycles, we have
\[
	[T]_\beta^\beta = \begin{bmatrix}
	J_{k_1}(\lambda_1)	& 0 	 & \cdots	& 0 \\
	0					& \ddots & \ddots	& \vdots \\
	\vdots				& \ddots & \ddots 	& 0 \\
	0					& \cdots & 0 		& J_{k_l}(\lambda_l)
	\end{bmatrix}
\]
This matrix is called the \textbf{Jordan Canonical Form} of $T$.

\begin{theorem}[Jordan Decomposition Theorem (JDT)]	
	There exists a Jordan basis w.r.t. $T$ for $V$ iff. the prime factors of $m_T$ have degree one. For whatever two bases of Jordan of $V$ w.r.t. $T$, the quantity of Jordan $T$-cycles of size $k$ and eigenvalue $\lambda$ are the same whatever $\lambda$ and $k$.
\end{theorem}

How do we find a Jordan basis? If the $T$-primary subspaces of $V$ are $T$-cyclic and we chose a generator for each $T$-cycle, the PDT and the previous discussion show how to find it.

What to do if any $T$-primary subspace is not $T$-cyclic? Or if some prime factor of $m_T$ has a degree greater than one? We use the Cyclic Decomposition Theorem since a cyclic decomposition always exists.

\begin{theorem}[Cyclic Decomposition Theorem (CDT)]\label{thm:CDT}
	There exist $m \in \mathbb{Z}, m \geq 1$, and $v_1, \ldots, v_m \in V \setminus \{ 0 \}$ such that
	\[
		V = \bigoplus_{j=1}^m C_T(v_j)
	\]
	and $m_{v_{j+1}} | m_{v_j}$ for all $1 \leq j < m$.
	
	If $u_1, \ldots, u_l \in V \setminus \{ 0 \}$ satisfying $V = \bigoplus_{j=1}^l C_T(u_j)$ and $m_{u_{j+1}} | m_{u_j}$ for all $1 \leq j < l$, then $l = m$ and $m_{u_j} = m_{v_j}$ for all $1 \leq j \leq m$.
\end{theorem}

The polynomials $m_{v_j}$ are called the \textbf{invariant factors} of $T$.

Recall that if $m_v(t) = t^k - \sum_{j=0}^{k-1} a_j t^j$, then $\mathfrak{C}_T(v) = v_0, \ldots, v_{k-1}$ with $v_j = T^j(v)$ is a basis of $C_T(v)$.

If $S$ is the induced operator by $T$ on $C_T(v)$, we have
\[
	[S]_{\mathfrak{C}_T(v)}^{\mathfrak{C}_T(v)} = \begin{bmatrix}
		0 		&& \cdots && \cdots && 0		&& a_0 \\
		1 		&& \ddots && 		&& \vdots	&& a_1 \\
		0 		&& \ddots && \ddots && \vdots	&& \vdots \\
		\vdots 	&& \ddots && \ddots && 0		&& \vdots \\
		0 		&& \cdots && 0 		&& 1		&& a_{k-1}
	\end{bmatrix}
\]
This matrix is called the \textbf{Frobenius matrix} (or \textbf{companion matrix}) of the polynomial $m_v$. A basis formed by the union of sets of the form $\mathfrak{C}_T(v)$ is said to be a \textbf{rational basis} (or \textbf{Frobenius basis}) of $V$ w.r.t. $T$.
	
Notice that $m_T = m_{v_1}$. In fact, we know that $m_{v_1} | m_T$ and, hence, it is sufficient to show that $m_{v_1} \in \mathfrak{A}_T$. By the CDT, given $w \in V$, there exist unique $w_j \in C_T(v_j)$, $1 \leq j \leq m$ such that $w = w_1 + \cdots + w_m$. We need to show that $m_{v_1}(T)(w_j) = 0$ for all $j$. However, $w_j \in C_T(v_j)$ implies that $m_{v_j}(T)(w_j) = 0$. Therefore, the result follows since $m_{v_j} | m_{v_1}$ for all $j$.

\begin{lemma}
	If $V = \bigoplus_{j=1}^m V_j$, $V_j$ is $T$-invariant for all $j$ and $T_j$ is induced by $T$ on $V_j$. Show that $c_T = \prod_{j=1}^m c_{T_j}$.
\end{lemma}

\begin{proof}
	\textbf{Exercise.}
\end{proof}
	
Thus, $c_T$ is the product of invariant factors.

If $T$ has a JCF, its distinct eigenvalues are $\lambda_j$, $1 \leq j \leq l$, and $k_j$ is the sum of the sizes of all Jordan blocks with eigenvalue $\lambda_j$, then
\[
	c_T = \prod_{j=1}^l (t - \lambda_j)^{k_j}
\]

The amount of Jordan blocks with eigenvalue $\lambda$ is equal to $\dim(V_{t-\lambda})$.

How are cyclic decomposition and Jordan decomposition related? 

\begin{lemma}
	If $S$ is the induced operator by $T$ on $V_{t-\lambda}^\infty$, a Frobenius basis w.r.t. $S - \lambda I$ is a Jordan basis w.r.t. $S$.
\end{lemma}

\begin{proof}
	\textbf{Exercise.}
\end{proof}

Now, given a Jordan basis, how do we find a rational basis? Let $\lambda_j$, $1 \leq j \leq l$, the distinct eigenvalues, and $m_j$ the amount of blocks with eigenvalue $\lambda_j$, and $m = \max \{ m_j : 1 \leq j \leq l \}$. Let $v_{j,1}, \ldots, c_{j,m_j}$ be vectors that start the Jordan $T$-cycles with eigenvalue $\lambda_j$ of a Jordan basis and suppose that they are ordered in such a way that $m_{v_{j, i+1}} | m_{v_{j,i}}$.

If $m_j < i \leq m$, define $v_{j,i} = 0$. Also
\[
	v_i = v_{1,i} + \cdots + v_{l,i}
\]

\begin{lemma}
	\[
		V = \bigoplus_{i=1}^m C_T(v_i) \quad \text{ and } \quad m_{v_{i+1}} | m_{v_i}, \quad \forall ~1 \leq i \leq m
	\]
\end{lemma}

\begin{proof}
	\textbf{Exercise.}
\end{proof}

\textbf{Exercise.} How can we reverse the result? I.e., given a Rational Form, how can we find a Jordan Canonical Form?

\begin{definition}[Similar Operators]
	Given two operators $S, T \in \text{End}(V)$. We say that $S$ and $T$ are \textbf{similar} if there exist $\alpha$ and $\beta$ bases of $V$ such that $[S]_\alpha^\alpha = [T]_\beta^\beta$.
\end{definition}

Remark that similarity is an equivalence relation.

\begin{theorem}
	$S$ and $T$ are similar iff. they have the same Rational Form. If $T$ has a Jordan Form, then $S \sim T$ iff. they have the same Jordan Form.
\end{theorem}

\begin{example}
	Let us continue the example \ref{ex:canonical_forms_1}. To generate the Jordan cycle associated with the eigenvalue $\lambda = 2$, we choose $v \in V_{(t-2)^3} \setminus V_{(t-2)^2}$, say $v = (1, 0, 0, -2)$. In this case, the Jordan cycle is
	\[
		v_1 = v, \quad v_2 = T(v_1) - 2v_1 = -2(2,1,1,1), \quad v_3 = T(v_2) - 2v_2 = -4 e_2
	\]
	
	To complete the basis, notice that $V_t = [v_4]$, in which $v_4 = (1, -1, 0, 0)$. Hence, $\beta = \{ v_1, v_2, v_3, v_4 \}$ is a Jordan basis and 
	\[
		[T]_\beta^\beta = \begin{bmatrix}
			2 && 0 && 0 && 0 \\
			1 && 2 && 0 && 0 \\
			0 && 1 && 2 && 0 \\
			0 && 0 && 0 && 0
		\end{bmatrix}
	\]
	
	Since $c_T = m_T$, it follows that $V$ is $T$-cyclic and $C_T(w) = V$, in which $w = v_1 - v_4 = (0, 1, 0, -2)$. Hence, the vectors $w_1 = w$,
	\[
		w_2 = T(w_1) = -2(1,1,1,3), \quad w_3 = T(w_2) = -4(3,3,2,4), \quad w_4 = T(w_3) = -8(5,6,3,5)
	\]
	form a rational basis for $V$ w.r.t. $T$.
	
	Given that $m_w(t) = c_T(t) = t^4 - 6t^3 + 12t^2 -8t$, it follows that
	\[
		T(w_4) = 6w_4 - 12 w_3 + 8 w_2
	\]
	
	Thus, taking the basis $\gamma = \{ w_1, w_2, w_3, w_4 \}$
	\[
		[T]_\gamma^\gamma = \begin{bmatrix}
			0 && 0 && 0 && 0 \\
			1 && 0 && 0 && 8 \\
			0 && 1 && 0 && -12 \\
			0 && 0 && 1 && 6
		\end{bmatrix}
	\]
\end{example}

How do we choose the initial vectors of each cycle? Jordan and Frobenius bases are formed by a union of cycles. Each cycle has an initial vector. How do we find it? This choice cannot be random.

\begin{example}
	Consider $T \in \text{End}(\mathbb{R}^3)$ given by $T(x,y,z) = (0,x,y)$. If $\alpha$ is the standard basis, we have 
	\[
		[T]_\alpha^\alpha = \begin{bmatrix}
			0 && 0 && 0 \\
			1 && 0 && 0 \\
			0 && 1 && 0
		\end{bmatrix}
	\]
	which is a Jordan block. Thus we can take $e_1$ as the initial vector of the (unique) cycle.

	Notice that $v = e_1 + e_2$ can also be used as an initial vector and we have the cycle $\beta = \{ v, e_2 + e_3, e_3 \}$. However, if the $x$-coordinate of the initial vector $w$ is zero, then this vector cannot be chosen, since $\mathfrak{C}_T(w)$ has at most two vectors and the final one is a multiple of $e_3$. 

	Finding bases for $V_t$ and $V_{t^2}$ we can see that there doesn't exist $T$-invariant subspace $W$ satisfying $\mathbb{R}^3 = C_T(w) \oplus W$. I.e., $C_T(w)$ doesn't admit a complementary $T$-invariant subspace.
\end{example}

Our task, then, is to pick $v_1$ such that $C_T(v_1)$ has a complementary subspace that can be written as a direct sum of cycles, hence $T$-invariant.

After that, we find $v_2$ such that $C_T(v_1) \cap C_T(v_2) = \{ 0 \}$ and $C_t(v_1) \oplus C_T(v_2)$ has a complementary subspace such that it can be written as a direct sum of cycles, hence $T$-invariant. 

Repeating this procedure, we find a rational basis.

\begin{definition}[$T$-admissible subspace]
	A $T$-invariant subspace $W$ of $V$ is said to be \textbf{$T$-admissible} if there exists a $T$-invariant subspace $W'$ such that $V = W \oplus W'$.
\end{definition}

\begin{example}
	$V$ and $\{ 0 \}$ are $T$-admissible. If $V$ is $T$-cyclic and $v$ satisfies $m_v = m_T$, then $V = C_T(v)$ and we can find a decomposition as in the \hyperref[thm:CDT]{CDT}.
\end{example}

To prove the Cyclic Decomposition Theorem, we need the following concept.

\begin{definition}[Conductor]
	Let $W$ be an invariant subspace of $T$ and $v \in V$. The set
	\[
    \mathfrak{C}_{T,v}(W) = \{ p \in \mathbb{F}[x] : p(T)(v) \in W \}
	\]
	is called the \textbf{$T$-conductor of $v$ into $W$}.

	If $W = \{ 0 \}$, the conductor is called the \textbf{$T$-annihilator of $v$}.
\end{definition}

Notice that $m_v$ divides every element of $\mathfrak{A}_{T,v}$ and there exists an unique monic polynomial $c_{v,W}$ that divides every element of $\mathfrak{C}_{T,v}(W)$.

It follows that $c_{v,W} | c_{v,W'}$ if $W' \subseteq W$. Therefore, $c_{v,W} | m_v$. If $v \in W$, then $c_{v,W} = 1$.

\begin{definition}
  A $T$-invariant subspace $W$ of $V$ is said to be \textbf{$T$-admissible} iff. for all $v \in V$ and $f \in \mathfrak{C}_{T,v}(W)$, there exists $w \in W$ such that $f(T)(w) = f(T)(v)$.
\end{definition}

\begin{lemma}\label{lm:202211141448}
  Let $v$ be as in the second item of the Theorem \ref{thm:202211141330}. Then, for all $u \in V$ satisfying $C_T(u) \cap (W \oplus C_T(v)) = \{0 \}$, $m_u | m_v$.
\end{lemma}

\begin{lemma}\label{lm:202211141449}
  If $v \in V$, then $C_T(v) \cap W = \{ 0 \}$ (i.e. is a direct sum) iff. $m_v = c_{v,W}$.
\end{lemma}

\begin{lemma}\label{lm:202211141450}
  Let $W$ be a $T$-invariant subspace of $V$ and suppose that $u, v \in V$ such that $v-u \in W$. Then $\mathfrak{C}_{T,v}(W) = \mathfrak{C}_{T,u}(W)$.
\end{lemma}

\begin{proof}
  Let $w = v-u$ and notice that 
  \[
    f(T)(v) - f(T)(u) = f(T)(w) \in W \quad \forall~f \in \mathbb{F}[x]
  \]
  Hence, $f \in \mathfrak{C}_{T,u}(W)$ iff. $f \in \mathfrak{C}_{T,v}(W)$.
\end{proof}

\begin{theorem}\label{thm:202211141330}
  Let $W$ be a proper subspace and $T$-admissible of $V$. Then the following are true:
  \begin{enumerate}
    \item For all $v \in V \setminus W$, there exists $v' \in V$ such that $m_{v'} = c_{v,W}$ and \[ W + C_T(v) = W \oplus C_T(v') \] 
    \item $W + C_T(v)$ is $T$-admissible if $v \in V$ satisfies $C_T(v) \cap W = \{ 0 \}$ and \[ \deg(m_v) = \max \{ \deg(m_u) : u \in V, C_T(u) \cap W = \{ 0 \} \} \]
  \end{enumerate}
\end{theorem}

\begin{proof}
  $(1)$ Using the definition of $T$-admissible, let $w \in W$ such that $c_{v,W}(T)(v) = c_{v,W}(T)(w)$ and consider $v' = v-w$. 

  Since $W, C_T(v)$, and $C_T(v')$ are $T$-invariant,
  \[
    W + C_T(v) = W + C_T(v')
  \]
  and we need to show that $W \cap C_T(v') = \{ 0 \}$.

  By the Lemma \ref{lm:202211141450}, $c_{v,W} = c_{v',W}$ and 
  \[
    c_{v',W}(T)(v') = c_{v,W}(T)(v-w) = 0
  \]
  
  Hence, $c_{v',W} \in \mathfrak{A}_{T,v'}$ and thus $c_{v', W} = m_{v'}$. By the Lemma \ref{lm:202211141449}, $C_T(v') \cap W = \{ 0 \}$. Remark that $c_{v,W}$ is the minimal polynomial in the space $V/W$.

  $(2)$ Let $W' = W + C_T(v)$. If $W' = V$, there is nothing to do. Suppose that $W \subsetneq V$ and take $u \in V \setminus W'$. Let $w \in W$ and $p \in \mathbb{F}[x]$ such that 
  \[
    c_{u, W'}(T)(u) = w + p(T)(v)
  \]
  
  We're going to show that 
  \begin{equation}\label{eq:202211170928}
    c_{u, W'} | p \quad \text{ and } \quad w = c_{u,W'}(T)(w') \text{ for some } w' \in W
  \end{equation}

  Writing the division of $p$ by $c_{u, W'}$, we have that $p = qc_{u,W'} + r$. Define $u' := u - q(T)(v) \in V \setminus W'$. By the Lemma \ref{lm:202211141450}, $c_{u, W'} = c_{u',W'}$.   

  By the part $(1)$ and choice of $v$, $\deg(m_v) \geq \deg(c_{u',W})$, and
  \begin{equation*}
  \begin{aligned}
  	c_{u, W'}(T)(u') &= c_{u, W'}(T)(u - q(T)(v)) = c_{u, W'}(T)(u) - (p(T) - r(T))(v) \\
	&= w + p(T)(v) - (p(T) - r(T))(v) = w + r(T)(v)
  \end{aligned}
  \end{equation*}
  
  On the other hand, there exists $h \in \mathbb{F}[x]$ such that $c_{u', W} = h c_{u', W'}$. Hence,
  \begin{equation*}
  \begin{aligned}
	  c_{u',W}(T)(u') = h(T)(c_{u,W'} (T(u'))) = h(T)(w + r(T)(v))
  \end{aligned}
  \end{equation*}
  and thus $hr \in \mathfrak{C}_{T,v}(W)$.
  
  If $r \neq 0$, then $\deg(h) + \deg(r) \geq \deg(c_{v,W}) = \deg(m_v) \geq \deg(c_{u',W}) = \deg(h) + \deg(c_{u,W'})$ and, therefore, $\deg(r) \geq \deg(c_{u,W'})$, which is impossible. Hence, $r = 0$ and the first part of \eqref{eq:202211170928} follows.
  
  Now notice that
  \[
  	c_{u,W'}(T)(u') = w + r(T)(v) = w \in W
  \]
  shows that $c_{u,W'} \in \mathfrak{C}_{T, u'}(W)$. Given that $W$ is $T$-admissible, there exists $w' \in W$ such that $c_{u,W'}(T)(w') = c_{u,W'}(T)(u') = w$, proving the second part \eqref{eq:202211170928}.

  Take $q \in \mathbb{F}[x]$ such that $p = q c_{u,W'}$ and $w'' := w' + q(T)(v) \in W'$. Then
  \[
    c_{u,W'}(T)(u) = c_{u, W'}(T)(w') + c_{u, W'}(T)(q(T)(v)) = c_{u,W'}(T)(w'')
  \]
  and hence $W'$ is $T$-admissible.
\end{proof}

\begin{theorem}[Cyclic Decomposition Theorem (Existence Part)]
	If $W$ is the proper subspace and $T$-admissible of $V$, there exist $m \in \mathbb{Z}_{\geq 1}$ and $v_1, \ldots, v_m \in V \setminus \{ 0 \}$ satisfying
	\begin{enumerate}
		\item The sum $W' = \sum_{j=1}^m C_T(v_j)$ is direct and $V = W \oplus W'$;
		\item $m_{v_{j+1}} | m_{v_j}, \quad \forall ~1 \leq j < m$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	We proceed by induction on $k := \dim(V) - \dim(W) \geq 1$. If $k = 1$, it is immediate by the previous result. In this case, $m = 1$ and $v_1 = v'$ given by the Theorem \ref{thm:202211141330} is an eigenvector.
	
	Suppose that $k > 1$ and, by induction hypothesis, the theorem is true for any subspace $T$-admissible $U$ of $V$ satisfying $\dim(V) - \dim(U) < k$. Choose $v_1$ as in the second item of the Theorem \ref{thm:202211141330} and let $U = W \oplus C_T(v_1)$, which satisfies our conditions.
	
	Thus, there exist $v_2, \ldots, v_m$ such that $V = U \oplus C_T(v_2) \oplus \cdots \oplus C_T(v_m)$ and $m_{v_{j+1}} | m_{v_j}$ for all $2 \leq j < m$. The Lemma \ref{lm:202211141448} gives that $m_{v_2} | m_{v_1}$.
\end{proof}

\begin{lemma}\label{lm:202211170945}
	Let $S, T \in \text{End}(V)$ and suppose that $S$ is invertible and $S \circ T = T \circ S$. Then, $m_{T,S(v)} = m_{T,v}$ for all $v \in V$.
\end{lemma}

\begin{proof}
	Notice that 
	\[
		m_v(T)(S(v)) = S(m_v(T)(v)) = 0
	\]
	shows that $m_{S(v)} | m_v$. 
	
	On the other hand, let $u = S(v)$ and recall that $S^{-1} \circ T = T \circ S^{-1}$. Then
	\[
		m_{S(v)}(T)(v) = m_u(T)(S^{-1}(u)) = S^{-1}(m_u(T)(u)) = 0
	\]
\end{proof}

\begin{lemma}\label{lm:202211170946}
	If $f \in \mathbb{F}[x], v \in V, u = f(T)(v)$, and $p = \gcd(f, m_v)$, then $m_v = p m_u$.
\end{lemma}

\begin{proof}
	Suppose wlog that $V = C_T(v)$. Otherwise, apply the following argument to the induced operator $T$ on $C_T(v)$.
	
	If $f$ and $m_v$ are coprime, then we shows that $m_u = m_v$. If $S = f(T)$, then $S \circ T = T \circ S$ and $S$ is bijective (since $\dim(V) < \infty$). The conclusion follows from the Lemma \ref{lm:202211170945}.
	
	Now suppose that $f$ is prime. if $f$ does not divide $m_v$, then $\gcd(f, m_v) = 1$ and we're back to the previous case. If $f = \gcd(f, m_v)$, we show that $m_u = g$, where $g = m_v / f$.
	\[
		g(T)(u) = g(T)(f(T)(v)) = (gf)(T)(v) = m_v(T)(v) = 0
	\]
	showing that $m_u | g$. If $m_u \neq g$, then 
	\[
		\deg(m_u) < \deg(g) = \deg(m_v) - \deg(f)
	\]
	
	However,
	\[
		(m_u f)(T)(v) = m_u(T)(f(T)(v)) = m_u(T)(u) = 0
	\]
	shows that $m_v | m_u f$. Thus
	\[
		\deg(m_v) \leq \deg(m_u) + \deg(f)
	\]
	gives a contradiction. Hence, $m_u = g$.
	
	In the general case, we proceed by induction on $\deg(f) \geq 1$, which starts in the preceeding case. Suppose that $\deg(f) > 1$ and let $h$ be an irreducible factor of $f$, and $g$ be such that $hg = f$, $u' = g(T)(v)$, $q = \gcd(g, m_v)$, and $r = \gcd(h, m_{u'})$.
	
	Notice that $u = h(T)(u')$. By induction hypothesis,
	\[
		m_v = q m_{u'} \quad \text{ and } \quad m_{u'} = r m_u
	\]
	
	By the properties of greatest common divisor, it follows that $q r = p$.
\end{proof}

\begin{theorem}[Cyclic Decomposition Theorem (Uniqueness Part)]
  If $W$ is a proper and $T$-admissible subspace of $V$, and suppose that $v_1, \ldots, v_m \in V \setminus \{ 0 \}$ and $u_1, \ldots, u_l \in V \setminus \{ 0 \}$ satisfy
	\begin{enumerate}
    \item The sums $W' := \sum_{j=1}^m C_T(v_j)$ and $W'' := \sum_{j=1}^l C_T(u_j)$ are direct;
    \item $V = W \oplus W' = W \oplus W''$;
    \item $m_{v_{j+1}} | m_{v_j}, \quad \forall ~1 \leq j < m$, and $m_{u_{j+1}} | m_{u_j}, \quad \forall ~1 \leq j < l$.
	\end{enumerate}
  Then $l = m$ and $m_{v_j} = m_{u_j}$ for all $1 \leq j \leq m$.
\end{theorem}

\begin{proof}
	Suppose $m \leq l$ and notice that $l = m$ will follow if we show that 
	\begin{equation}
		m_{u_j} = m_{v_j} \quad \forall~1 \leq j \leq m
	 \label{eq:202211212038}
	\end{equation}
	
	In fact, these identities imply that
	\[
		\dim(V) = \dim(W) + \sum_{j=1}^m \dim(C_T(v_j)) = \dim(W) + \sum_{j=1}^m \dim(C_T(u_j))
	\]
  and therefore there can't be more elements at the decomposition. 

  Now consider $S_j := m_{v_j}(T)$, for $1 \leq j \leq m$. Applying $S_1$ to both decompositions, 
  \[
    S_1(W) \oplus S_1(C_T(u_1)) \oplus \cdots \oplus S_1(C_T(u_l)) = S_1(W)
  \]
  by definition of $S_1$ and the fact that $m_{v_{j+1}} | m_{v_j}$.

  Hence, $m_{v_1}(T)(u_1) = 0$, showing that $m_{u_1} | m_{v_1}$. In a similar way, we show that $m_{v_1} | m_{u_1}$ and, therefore, $m_{u_1} = m_{v_1}$.

  If $m = 1$, then \eqref{eq:202211212038} is proved. Otherwise, apply $S_1 = m_{v_2}(T)$ to obtain 
  \[
    S_2(W) \oplus S_2(C_T(u_1)) \oplus \cdots \oplus S_2(C_T(u_l)) = S_2(W) \oplus S_2(C_T(v_1)) 
  \]

  Since $f(T)(C_T(v)) = C_T(f(T)(v))$, for all $f \in \mathbb{F}[x]$ and $v \in V$,
  \[
    S_2(W) \oplus C_T(S_2(u_1)) \oplus \cdots \oplus C_T(S_2(u_l)) = S_2(W) \oplus C_T(S_2(v_1))
  \]

  By the Lemma \ref{lm:202211170946}, $m_{S_2(v_1)} = \frac{m_{v_1}}{m_{v_2}} = \frac{m_{u_1}}{m_{v_2}} = m_{S_2(u_1)}$. Hence, 
  \[
    \dim(C_T(S_2(v_1))) = \dim(C_T(S_2(u_1)))
  \]
  and $C_T(S_2(u_j)) = \{ 0 \}$, for all $j \geq 2$. Thus, $m_{v_2}(T)(u_2) = S_2(u_2) = 0$, showing that $m_{u_2} | m_{v_2}$. Analogously, $m_{v_2} | m_{u_2}$ and, therefore, $m_{u_2} = m_{v_2}$. 

  Proceding recursively using $S_j$, the \eqref{eq:202211212038} is proved.
\end{proof}

Using the Primary Decomposition Theorem and the Cyclic Decomposition Theorem, how can we obtain the Jordan Decomposition Theorem? 

Consider the $T$-primary decomposition of $V$, $V = V_1 \oplus \cdots \oplus V_m$, and obtain a decomposition of each $V_i$ as in the CDT: 
\[
  V_i = C_T(v_{i,1}) \oplus \cdots \oplus C_T(v_{i,l_i})
\]

Then $m_{v_{i,j}} = p_i^{k_{i,j}}$, where $k_{i,1} \geq \ldots \geq k_{i,l_i}$, in which $p_i$ is the corresponding prime factor of $m_T$. It follows that
\[
  V = \bigoplus_{i=1}^m \bigoplus_{j=1}^{l_i} C_T(v_{i,j})
\]  

If $p_i = t - \lambda_i$ for all $i$, and for each $v_{i,j}$, construct the corresponding Jordan $T$-cycle, which we'll call $\mathfrak{J}_{i,j}$. Thus, \[ \mathfrak{J} := \bigcup_{i,j} \mathfrak{J}_{i,j} \] is a Jordan basis of $V$ w.r.t. $T$.

Reciprocally, supposing that there exists a Jordan basis w.r.t. $T$ for $V$, and using such a basis to compute $c_T$, we conclude that $\deg(p_i) = 1$, for all $i$.

\begin{theorem}
  $S$ and $T$ are similar if, and only if, they have the same rational canonical form. If $T$ has a Jordan canonical form, then $S \sim T$ if, and only if, they have the same Jordan canonical form. 
\end{theorem}

\begin{proof}
  Consequence of the \hyperref[thm:CDT]{Cyclic Decomposition Theorem}.
\end{proof}

If $\mathbb{K}$ is a subfield $\mathbb{F}$, it may happen that $A$ and $B$ are similar over $\mathbb{F}$ but not over $\mathbb{K}$.

\begin{example}
  Let 
  \[
    A = \begin{bmatrix}
      0 & -1 \\
      1 & 0 
      \end{bmatrix}, \quad \begin{bmatrix}
      i & 0 \\
      0 & -i 
    \end{bmatrix}
  \]
  
  Then over $\mathbb{C}$, $c_A(t) = t^2 + 1 = (t-i)(t+i)$, and $A$ has the Jordan form 
  \[
    J_A = \begin{bmatrix} 
      i & 0 \\
      0 & -i 
    \end{bmatrix} = B
  \]

  Thus, $A$ and $B$ are similar over $\mathbb{C}$.

  Notice that the rational form of $A$ is $R_A = A$. Since the characteristic polynomial for $B$ is $c_B(t) = (t-i)(t+i) = c_A(t)$, the rational form of $B$ is 
  \[
    R_B = \begin{bmatrix}
      0 & -1 \\
      1 & 0 
    \end{bmatrix} = A
  \]

  Notice, however, that over $\mathbb{R}$ the matrices $A$ and $B$ are not similar
\end{example}
